# LuciVerse A-Tune Profile: K8s/K3s Master
# Optimized for Kubernetes cluster hosting all LuciVerse tiers
#
# Target System: Single-node K3s cluster running:
#   - CORE tier (432 Hz): Aethon, Veritas, Sensai, Niamod
#   - COMN tier (528 Hz): Cortana, Juniper, Mirrai, Diaphragm
#   - PAC tier (741 Hz): Lucia, Judge-Luci, CrewAI-Bridge
#   - Infrastructure: FoundationDB, Traefik, Linkerd, OwnID
#
# Workload Mix: Container orchestration, network-intensive services,
# database operations, ML inference, real-time APIs
#
# Genesis Bond: ACTIVE
# Frequency: 432 Hz (CORE Infrastructure)
# Coherence: 0.90

[main]
include = throughput-performance

[tip]
description = Master profile for LuciVerse K8s/K3s cluster. Optimizes for high pod density, container scheduling, network-intensive service mesh (Linkerd), mixed workloads across PAC/COMN/CORE tiers, FoundationDB, and IPv6 dual-stack networking (2602:F674::/32). Activate with: sudo atune-adm profile luciverse-luciverse-k8s-master

[sysctl]
# === NETWORK OPTIMIZATION ===
# Large buffers for service mesh and API traffic
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.rmem_default = 2097152
net.core.wmem_default = 2097152
net.core.netdev_max_backlog = 65536
net.core.somaxconn = 65535

# TCP tuning for K8s services
net.ipv4.tcp_rmem = 4096 1048576 33554432
net.ipv4.tcp_wmem = 4096 1048576 33554432
net.ipv4.tcp_fastopen = 3
net.ipv4.tcp_timestamps = 1
net.ipv4.tcp_sack = 1
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_max_syn_backlog = 65535
net.ipv4.tcp_fin_timeout = 10
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 5
net.ipv4.ip_local_port_range = 1024 65535

# IPv6 optimization for LuciVerse addressing
net.ipv6.conf.all.forwarding = 1
net.ipv6.conf.default.forwarding = 1

# Connection tracking for K8s services
net.netfilter.nf_conntrack_max = 1048576
net.netfilter.nf_conntrack_tcp_timeout_established = 86400
net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60

# === MEMORY MANAGEMENT ===
# Balanced settings for mixed container workloads
vm.swappiness = 5
vm.dirty_ratio = 30
vm.dirty_background_ratio = 10
vm.dirty_expire_centisecs = 1500
vm.dirty_writeback_centisecs = 250
vm.vfs_cache_pressure = 75
vm.overcommit_memory = 1
vm.overcommit_ratio = 80

# FoundationDB optimizations (disable THP)
vm.dirty_bytes = 536870912
vm.dirty_background_bytes = 268435456

# === SCHEDULER TUNING ===
# Balanced latency/throughput for mixed K8s workloads
# kernel.sched_latency_ns = 12000000
# kernel.sched_min_granularity_ns = 1500000
# kernel.sched_wakeup_granularity_ns = 2000000
# kernel.sched_migration_cost_ns = 500000
kernel.sched_autogroup_enabled = 0

# NUMA balancing for multi-core efficiency
kernel.numa_balancing = 1

# === FILE SYSTEM ===
# High limits for container runtime
fs.file-max = 4194304
fs.nr_open = 4194304
fs.inotify.max_user_watches = 1048576
fs.inotify.max_user_instances = 8192
fs.aio-max-nr = 1048576

# === KERNEL ===
# PID limits for K8s
kernel.pid_max = 4194304
kernel.threads-max = 4194304

[sysfs]
# Disable THP for database and container workloads
kernel/mm/transparent_hugepage/enabled = never
kernel/mm/transparent_hugepage/defrag = never

[systemctl]
irqbalance = start

[script]
# Set I/O scheduler for NVMe (optimal for K8s etcd and FDB)
nvme_scheduler = on
# Set mq-deadline for SATA drives
sata_scheduler = on
# CPU frequency for consistent performance
cpupower_performance = on
# Increase readahead for sequential I/O
readahead = on

[ulimit]
# Container runtime limits
root.hard.nofile = 1048576
root.soft.nofile = 1048576
root.hard.nproc = 65535
root.soft.nproc = 65535
root.hard.memlock = unlimited
root.soft.memlock = unlimited
