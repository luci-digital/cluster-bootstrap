# A-Tune Profile: LuciVerse ML Inference (Sensai/Semantic Engine)
# Genesis Bond: ACTIVE @ 432 Hz
#
# Optimized for ML inference workloads:
# - PyTorch/scikit-learn inference
# - Vector embeddings and similarity search
# - Batch processing with low latency
# - Memory-intensive operations

[main]
include = luciverse-luciverse-consciousness

[sysctl]
# Higher memory limits for ML
vm.swappiness = 1
vm.overcommit_memory = 1
vm.overcommit_ratio = 80

# Large page support
vm.nr_hugepages = 2048

# Faster task switching
# kernel.sched_min_granularity_ns = 1000000
# kernel.sched_wakeup_granularity_ns = 1500000

[sysfs]
# Aggressive readahead for model loading
block/{disk}/queue/read_ahead_kb = 4096
block/{disk}/queue/nr_requests = 512

[script]
# Maximum performance governor
cpupower_performance = on
# Disable transparent huge pages
disable_thp_enabled = on

[tip]
description = ML-specific optimizations for inference workloads. Pin inference threads to cores: taskset -c 0-5 python3 inference.py. Use MKL/OpenBLAS threading: export OMP_NUM_THREADS=6
